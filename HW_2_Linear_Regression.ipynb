{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2 CSCI 4364/6364 Machine Learning\n",
        "\n",
        "##**Adventures in Linear Regression**\n",
        "v.20220920a\n",
        "\n",
        "**Due Date: 10/4/2022, 23:59 ET**\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose:**\n",
        "This homework will familiarize you with linear regression using the [Prostate Cancer dataset](https://hastie.su.domains/ElemStatLearn/data.html). First, you’ll work with the least squares. Then you’ll investigate regression using L2 (Ridge) and L1 (Lasso) regularization. Finally, you will implement an iterative version of L2 Regularization using gradient descent.\n",
        "\n",
        "**Note**: Besides part 3, you should implement your solution with the fundamental equations we discussed in class and in Hastie, chapter 3. *Only in part 3, you should use Scikit-Learn*.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Submission Instructions:**\n",
        "This assignment will be done entirely in this Colaboratory notebook, and you will submit your notebook via GWU blackboard. Please embed your code in code blocks and add in comments into the text blocks. \n",
        "\n",
        "**Grading on the notebook:**\n",
        "\n",
        "Parts 1 - 4 of this notebook are worth 5% of the semester grade, where 3% is completion and full functionality, and 2% is based on comments and descriptions, and well-written and commented Python code, based on the coding standards. The notebook should be fully explained and work in its entirety when you submit it.\n",
        "\n",
        "**Extra Credit!** Besides being a great learning experience about convex optimization, part 5 is **optional**, but worth up to 3% of the semester grade. \n",
        "\n",
        "**Coding Standards:**\n",
        "Throughout this course, we will use Google’s Python Style Guide (https://google.github.io/styleguide/pyguide.html) as the coding standard for homework and project submission. A big part of machine learning in industry is applying good programming practices.\n"
      ],
      "metadata": {
        "id": "ris4FZxnH9iF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Anulekha\n",
        "\n",
        "**GW ID:** G40397446"
      ],
      "metadata": {
        "id": "z2iRXG9B5hPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "##########################################################\n",
        "# Always include all imports at the first executable cell.\n",
        "##########################################################\n",
        "from abc import ABC, abstractmethod # Abstract Base Classes for Python\n",
        "import pandas as pd # Pandas dataframe libaries\n",
        "import numpy as np # Numpy numerical computation library\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn import metrics # Used to compute metrics, such as the Area Under the Curve (AUC)\n",
        "import matplotlib.pyplot as plt # Plotting library.\n",
        "from typing import List, Tuple, Mapping # Common types for Python type definitions\n",
        "from sklearn import linear_model\n"
      ],
      "metadata": {
        "id": "DnXm2ekDUw3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preperation & Feature Analysis (Prostate Cancer)\n",
        "\n"
      ],
      "metadata": {
        "id": "pO4burfLcwLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b-2cRhwTybF"
      },
      "outputs": [],
      "source": [
        "#@title Load the Prostate Cancer dataset \n",
        "_SEED = 1223\n",
        "random_state = np.random.RandomState(_SEED)\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/prostate.csv')\n",
        "# Randomize the rows\n",
        "df = df.sample(frac =  1, random_state=random_state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "LKee42j0hzGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split into training and test set\n",
        "# Following Hastie p.50, we create a training set of 67\n",
        "split_index = 67\n",
        "df_train = df.iloc[:split_index]\n",
        "df_test = df.iloc[split_index:]"
      ],
      "metadata": {
        "id": "BYAUOdwhUwhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display pair plots\n",
        "sns.pairplot(df, kind=\"reg\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Wkl8unYc-S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split the labels and convert to numpy arrays\n",
        "y_train = df_train['lpsa'].to_numpy() # independent variable is 'lpsa'\n",
        "X_train = df_train.copy().drop(columns = ['lpsa']).to_numpy() # everything but the IV are the DV\n",
        "y_test = df_test['lpsa'].to_numpy()\n",
        "X_test = df_test.copy().drop(columns = ['lpsa']).to_numpy()"
      ],
      "metadata": {
        "id": "DGTFmioJY4gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseLearningAlgorithm(ABC):\n",
        "  \"\"\"Base class for a Supervised Learning Algorithm.\"\"\"\n",
        "\n",
        "  @abstractmethod\n",
        "  def train(self, X_train:np.array, y_train: np.array) -> None:\n",
        "    \"\"\"Trains a model from labels y and examples X.\"\"\"\n",
        "\n",
        "  @abstractmethod\n",
        "  def predict(self, X_test: np.array) -> np.array:\n",
        "    \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n",
        "\n",
        "  @property\n",
        "  @abstractmethod\n",
        "  def name(self) -> str:\n",
        "    \"\"\"Returns the name of the algorithm.\"\"\""
      ],
      "metadata": {
        "id": "L7xSPWoLQWYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Linear Regression with Least Squares\n",
        "Implement a class called `BasicLeastSquaresRegression` that extends `BaseLearningAlgorithm` with “vanilla” least squares regression described in Hastie 3.2. Compute the $\\boldsymbol{\\beta}$ coefficient vector and solve for $\\hat{y} $ and compute the performance result as the mean squared error on the test set. Use only numpy for your solution.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7_klHAmDNAyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicLeastSquaresRegression(BaseLearningAlgorithm):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._b = None\n",
        "\n",
        "  def train(self, X_train:np.array, y_train: np.array) -> None:\n",
        "    \"\"\"Trains a model from labels y and examples X.\"\"\"\n",
        "    self._b = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)   \n",
        "\n",
        "  def predict(self, X_test: np.array) -> np.array:\n",
        "    \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n",
        "    y_hat = X_test.dot(self._b)\n",
        "    return y_hat\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    \"\"\"Returns the name of the algorithm.\"\"\"\n",
        "    return \"ls\"\n",
        "\n",
        "  def MSE(self,y_test:np.array,y_hat:np.array):\n",
        "    err_sq = (y_test - y_hat)**2\n",
        "    mse = np.sum(err_sq)/y_test.shape[0]\n",
        "    return mse\n",
        "\n",
        "algo = BasicLeastSquaresRegression()\n",
        "algo.train(X_train, y_train)\n",
        "algo._b\n",
        "y_hat = algo.predict(X_test)\n",
        "print(y_hat)\n",
        "algo.MSE(y_test,y_hat)\n"
      ],
      "metadata": {
        "id": "NRA936ezJ6RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "**1.1** What MSE loss score do you get with Least Squares?\n",
        "\n",
        "\n",
        "\n",
        "**1.2** What variables carry the greatest influence (i.e., are the most important) in the least squares regression?\n",
        "\n",
        "\n",
        "\n",
        "**1.3** Does normalizing the data improve performance (lower MSE loss)? If so, why?\n"
      ],
      "metadata": {
        "id": "s2193UPuQ2qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Linear Regression with L2 Regularization (Ridge)\n",
        "\n",
        "Using the closed-form solution to Ridge Regression, implement a class called `RidgeRegression` that extends `BaseLearningAlgorithm` based on Hastie 3.41. Iterate through the regression penalty term, $\\lambda$, and plot (a) MSE loss as a function of $\\lambda$, and (b) each coefficient weight as a function of $\\lambda$. Use only numpy and matplotlib for your solution.\n"
      ],
      "metadata": {
        "id": "27dtemGJd6qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeRegression(BaseLearningAlgorithm):\n",
        "\n",
        "  def __init__(self, lambdap):\n",
        "    self._theta = None\n",
        "    self._lambdap = lambdap\n",
        "\n",
        "\n",
        "  def train(self, X_train:np.array, y_train: np.array) -> None:\n",
        "    \"\"\"Trains a model from labels y and examples X.\"\"\"\n",
        "    I = np.identity(X_train.shape[1])\n",
        "    I[0][0] = 0\n",
        "    self._theta = np.linalg.inv(X_train.T.dot(X_train) + self._lambdap * I).dot(X_train.T).dot(y_train)   \n",
        "\n",
        "  \n",
        "  def predict(self, X_test: np.array) -> np.array:\n",
        "    \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n",
        "    y_hat_ridge = X_test.dot(self._theta)\n",
        "    return y_hat_ridge\n",
        "\n",
        "  @property\n",
        "  def name(self) -> str:\n",
        "    \"\"\"Returns the name of the algorithm.\"\"\"\n",
        "    return \"ridge_reg\"\n",
        "\n",
        "  def MSE(self,y_test:np.array,y_hat_ridge:np.array):\n",
        "    err_sq = (y_test - y_hat_ridge)**2\n",
        "    mse = np.sum(err_sq)/y_test.shape[0]\n",
        "    print(mse)\n",
        "\n",
        "ridge = RidgeRegression(lambdap=5)\n",
        "ridge.train(X_train, y_train)\n",
        "ridge._theta\n",
        "print(ridge._lambdap)\n",
        "y_hat_ridge = ridge.predict(X_test)\n",
        "print(y_hat_ridge)\n",
        "ridge.MSE(y_test,y_hat_ridge)"
      ],
      "metadata": {
        "id": "VdBB7fgGuRJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "\n",
        "**2.1** What $\\lambda$ do you get the best performance (i.e., lowest MSE)?\n",
        "\n",
        "The $\\lambda$ with the best performance, or the lowest MSE, is 5. Anything below or higher results in a slightly higher MSE.\n",
        "\n",
        "**2.2** Compare the results from part 1, when $\\lambda = 0$. Why are the results similar or different?\n",
        "\n",
        "The results are slightly different (~0.55 for $\\lambda$=0 and ~0.54 for $\\lambda$=5) because the added bias \n",
        "\n",
        "**2.3** Compare the weights with the weights from part 1? If you were to rank descending by the absolute value of the weights, how different is the ordering with part 1? Is the most important variable in part 1 the same as in part 2? If not, can you provide a reason?\n",
        "\n"
      ],
      "metadata": {
        "id": "MmB1Zg4_RenB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Linear Regression with L1 Regularization (Lasso)\n",
        "\n",
        "Unlike Ridge Regression, there is no closed-form solution for Lasso, meaning there is no normal equation we can solve to immediately get all of our ideal model parameters. While it can be solved by minimizing one coordinate a time using a technique called [Coordinate Descent](http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-coordinate-descent-using-least-squares-regression/), here you can implement your solution using Scikit-Learn Lasso inside a class called `LassoRegression` which also extends `BaseLearningAlgorithm`. Like part 2, iterate through the regression penalty term, $\\alpha$ , and plot (a) MSE loss as a function of $\\alpha$, and (b) each coefficient weight as a function of $\\alpha$. \n",
        "\n",
        "**Note**: here we swap notation a little, replacing $\\lambda$ with $\\alpha$ to fit with the Scikit-Learn convention.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xy-PAEjbRypK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class LassoRegression(BaseLearningAlgorithm):\n",
        "\n",
        "#   def __init__(self, ):\n",
        "#     self._theta = None\n",
        "#     self._lambdap = lambdap\n",
        "\n",
        "\n",
        "#   def train(self, X_train:np.array, y_train: np.array) -> None:\n",
        "#     \"\"\"Trains a model from labels y and examples X.\"\"\"\n",
        "   \n",
        "  \n",
        "#   def predict(self, X_test: np.array) -> np.array:\n",
        "#     \"\"\"Predicts on an unlabeled sample, X.\"\"\"\n",
        "    \n",
        "\n",
        "#   property\n",
        "#   def name(self) -> str:\n",
        "#     \"\"\"Returns the name of the algorithm.\"\"\"\n",
        "#     return \"lasso_reg\""
      ],
      "metadata": {
        "id": "U6DsekZm32wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = Lasso(alpha=1.0)\n",
        "lasso.fit(X_train,y_train)\n",
        "lasso.score(X_test,y_test), lasso.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "b3VZ5wqbSXrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_yhat = lasso.coef_\n",
        "lasso_yhat"
      ],
      "metadata": {
        "id": "j4BuQeZ3UNrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_lasso = np.asarray(lasso_yhat)\n",
        "err_sq = (y_test - y_hat_lasso)**2\n",
        "mse = np.sum(err_sq)/y_test.shape[0]\n",
        "print(mse)"
      ],
      "metadata": {
        "id": "iBYQq7AQUdMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "**3.1** Under what conditions would you prefer L2 or L1 regression over vice versa? (Consider the discussion in Hastie 3.6.)\n",
        "\n",
        "Both L1 and L2 regularization can help with multicollinearity.\n",
        "\n",
        "L1 regularization can be used as a feature selection since it pins down coefficients to 0, while L2 constraints the coefficients and still keep all the variables. \n",
        "\n",
        "L1 is good for sparsity as it encourages 0 coefficients. It is useful for minimising the number of inputs.\n",
        "\n",
        "L2, on the other hand, keeps all of the variables, therefore it is more useful when dealing with collinearity.\n",
        "\n",
        "**3.2** Why do some coefficients become zero? Do you think this may be a method of subset selection as described in Hastie 3.3?\n",
        "\n",
        "Some coefficients are set to zero in order to make the overall prediction better. Least squares method often has low bias but high variance. Setting coefficients to zero might increase bias slightly, but reduces variance, making for an overall better prediction. \n",
        "\n",
        "Yes. This is method of subset selection. \n",
        "\n",
        "**3.3** Which method performs better (i.e., has the lower MSE)?\n",
        "\n",
        "L1 regression has the lower MSE and therefore performs better in this example.\n",
        "\n",
        "**3.4** Comparing the relative ranking of the weights at the lowest MSE, do Ridge Regression and Lasso Regression “agree” on the most important weights?\n",
        "\n",
        "**3.5** In your own words, define **bias** and **variance**. Describe how bias and variance affected the results of parts 1, 2, and 3.\n",
        "\n",
        "Bias is how much the model's predictions differ from the target value compared to the training data. \n",
        "\n",
        "Variance describes how much the predictions will differ if a different training data was used. \n",
        "\n",
        "Added bias in the least squares method is 0. Increasing the bias by a $\\lambda$ factor of 5 in the ridge regression in part 2 improves the model by reducing variance. This leads to a lower MSE score. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zza9NC3LSK46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.  Iterative Optimization with Gradient Descent\n",
        "Implement a class called `RidgeRegressionGradDescent` that extends `BaseLearningAlgorithm` that performs linear regression with L2 regularization using [gradient descent](https://www.deeplearningbook.org/contents/numerical.html), Goodfellow 4.5. Plot both training and test MSE loss result vs. iteration.  Iterate through the regression penalty term, $\\lambda$ , and plot (a) MSE loss as a function of $\\lambda$, and (b) each coefficient weight as a function of $\\lambda$. Use only numpy and matplotlib for your solution.\n"
      ],
      "metadata": {
        "id": "1m_aTGuhSYFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "**4.1** How many iterations are required to converge, and are there any performance differences compared to part 2?\n",
        "\n",
        "**4.2** Derive the gradient of L2 regularization loss. Encode your answer as [$\\LaTeX$ equations](https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/00/intro_to_latex.ipynb), and justify every step, please.\n",
        "\n",
        "**4.3** Do you converge on the same minimum loss value? How do the coefficients compare to part 1?\n"
      ],
      "metadata": {
        "id": "mYiC_VJ_Ss8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. [OPTIONAL] Iterative Optimization using Coordinate Descent \n",
        "\n",
        "**Worth up to 3% of the semester grade.**\n",
        "\n",
        "Review Hastie 3.8.6 and this [presentation](https://www.cs.cmu.edu/~pradeepr/convexopt/Lecture_Slides/coordinate_descent.pdf) on coordinate descent. Using just numpy reimplement part 3 with coordinate descent. Like part 3, iterate through the regression penalty term, $\\lambda$, and plot (a) MSE loss as a function of $\\lambda$, and (b) each coefficient weight as a function of $\\lambda$. Use only numpy and matplotlib for your solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "kcKViVOgXbBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "**5.1** How does your implementation compare with Scikit-Learn’s implementation in terms of minimum MSE loss and coefficients?\n",
        "\n",
        "**5.2** Is the optimization surface convex or non-convex? \n",
        "\n",
        "**5.3** What makes the optimization surface *non-smooth* and how is coordinate descent able to overcome this problem?\n"
      ],
      "metadata": {
        "id": "wEGQVz2AX3b7"
      }
    }
  ]
}
