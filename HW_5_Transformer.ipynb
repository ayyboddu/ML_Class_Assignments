{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ris4FZxnH9iF"
      },
      "source": [
        "# Homework 5 CSCI 4364/6364 Machine Learning\n",
        "\n",
        "##**Language Models with Attention and Transformer**\n",
        "\n",
        "**Due Date: 12/16/2022, 23:59 ET**\n",
        "\n",
        "---\n",
        "\n",
        "**Purpose:**\n",
        " This homework assignment will give you experience working with the Transformer architecture on a text summarization model. You will learn about generating vocabularies using subword tokenization. In real-world applications, there is a good chance that you will have to repurpose an ML architecture into a different task. Similarly, in this assignment you will convert the Portuguese-English Transformer example into a summarizer. While this is a relatively small language model, interacting with the Transformer will give you an intuition for some of the large language models (LLMs), like GPT3, Pathways Language Model (PaLM), LaMDA and others. You will review attention weights, and evaluate the quality of your summarizer using the ROUGE score, which is a common but imperfect method of evaluating the quality of summaries.\n",
        "\n",
        "Before you dive into this homework assignment, you should review the following papers and tutorials:\n",
        "* **Attention Is all you need** (https://arxiv.org/abs/1706.03762) This is the original paper that will provide a description the design of the Transformer architecture.\n",
        "* **Neural machine translation with a Transformer and Keras** (https://www.tensorflow.org/text/tutorials/transformer) This is the Tensorflow/Keras language translation model that you will convert from a translator into a summarizer.\n",
        "* **Subword Tokenizer** (https://www.tensorflow.org/text/guide/subwords_tokenizer) This tutorial will provide you the background to implement your own sub-words tokenizer for the summarization task.\n",
        "* **Introduction to Text Summarization with ROUGE Scores** (https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471) This post will familiarize you with the abstractive summarization task and present the ROUGE score metric, based on precision and recall of words compared to the training set. \n",
        "\n",
        "---\n",
        "**Important!** \n",
        "\n",
        "* The models you are working with in this homework are quite advanced and require some time and TPU resources to execute. You may consider upgrading to [Colab Pro](https://colab.research.google.com/signup) or the pay-as-you-go subscription if you are running out of memory or if your session disconnects after a while. As you proceed through the assignment, take note of the overall **accuracy**, **runtime**, and the **size** (number of parameters) of your models. Finally, give yourself some time to execute the code in the homework.\n",
        "* You should run this Colab with a Tensor Processor Unit (**TPU**) runtime with **High RAM** configuration. \n",
        "\n",
        "---\n",
        "**Submission Instructions:**\n",
        "This assignment will be done entirely in this Colaboratory notebook, and you will submit your notebook via GWU blackboard. Please embed your code in code blocks and add in comments into the text blocks. \n",
        "\n",
        "**Grading on the notebook:**\n",
        "This notebook is worth up to 5% (with up to 3% additional extra credit) of the semester grade, where 3% is completion and full functionality, and 2% is based on comments and descriptions, and well-written and commented Python code, based on the coding standards. The notebook should be fully explained and work in its entirety when you submit it.\n",
        "\n",
        "**Coding Standards:**\n",
        "Throughout this course, we will use [Google’s Python Style Guide](https://google.github.io/styleguide/pyguide.html) as the coding standard for homework and project submission. A big part of machine learning in industry is applying good programming practices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2iRXG9B5hPK"
      },
      "source": [
        "**Name:** Anulekha Boddu\n",
        "\n",
        "**GW ID:** G40397446"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup Installation\n",
        "!pip install rouge-score\n",
        "!pip install -q -U \"tensorflow-text==2.8.*\"\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow\n",
        "print(\"NOTE: You may have to restart the runtime after this completes.\")\n"
      ],
      "metadata": {
        "id": "b-m3qVYUf3mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFCtHBexYXxo"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "##########################################################\n",
        "# Always include all imports at the first executable cell.\n",
        "##########################################################\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from typing import Dict, Optional, Sequence\n",
        "import tensorflow as tf\n",
        "from gc import callbacks\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "from google.colab import drive # For saving vocabulary and model checkpoints.\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "from IPython.display import display, HTML\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The section below sets up folders for the vocabulary and transformer weight checkpoints on Google Drive (gDrive). This ensures that your models are saved if the colab session crashes, and you can recover an existing model from the latest checkpoint. See the [tutorial on colab IO](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA) for more details on I/O with colabs."
      ],
      "metadata": {
        "id": "Of0-rSdxZA9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up the vocabulary and checkpoint paths on gDrive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "checkpoint_filepath = F'/content/gdrive/My Drive/ml_course/hw5/models/summarizer/checkpoints'\n",
        "vocabulary_filepath =  F'/content/gdrive/My Drive/ml_course/hw5/models/summarizer/vocabulary'\n",
        "vocabulary_file = os.path.join(vocabulary_filepath, 'vocabulary')\n",
        "\n",
        "# For saving the vocabulary\n",
        "if not os.path.exists(vocabulary_filepath) :\n",
        "  print('Creating vocabulary file path %s' %vocabulary_filepath)\n",
        "  os.makedirs(vocabulary_filepath)\n",
        "\n",
        "# For saving the model weights\n",
        "if not os.path.exists(checkpoint_filepath) :\n",
        "  print('Creating checkpoint directory %s' %checkpoint_filepath)\n",
        "  os.makedirs(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "aYW-FERkqBHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HQCFT2TrhIE"
      },
      "source": [
        "#1. Access the SummScreen dataset from TFDS\n",
        "\n",
        "The [SummScreen dataset](https://www.tensorflow.org/datasets/catalog/summscreen#summscreentms) is  a collection of scripts and summaries from various TV shows. The goal is to analyze a portion of the script  to generate an abstractive summary. While a larger model might be able to ingest a large portion of the text, we will apply a very conservative limit on the input length. Since  will use the raw plain text input, we’ll need to create a vocabulary and tokenize the data before training a sequence-to-sequence model. \n",
        "\n",
        "As in previous assignments, you'll be working on **TensorFlow datasets**, so you may want to review the [API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Specifically, you should review `map()`, `shuffle()`, `batch()`, and `load()`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\"summscreen\", \n",
        "                                           as_supervised=True, with_info=True, split = ['train', 'validation', 'test'])\n",
        "df = tfds.as_dataframe(ds_train.shuffle(1000).take(20), ds_info)\n",
        "display(HTML(df.iloc[:3].to_html()))"
      ],
      "metadata": {
        "id": "zdSiOyP2S8kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create a vocabulary from the training script input data\n",
        "\n",
        "In this section, use the training scripts to create a vocabulary using `bert_vocab.bert_vocal_from_dataset()`.  You should create DFDS mapping functions that convert the transcripts to lowercase and add `[START]` and `[END]` tokens to transcripts. Unlike the translation task, you will only have to worry about a single language vocabulary and tokenization, since the word-piece encoder works for both input (transcript) and output (recap)."
      ],
      "metadata": {
        "id": "19-hqbYkR7X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1 Sub-word vocabulary\n",
        "Describe the advantages a sub-word vocabulary provides over a vocabulary of the same length made up of the most frequently occuring whole words.\n",
        "\n",
        "\n",
        "**Answer**\n",
        "\n",
        "\n",
        "An advantage of the sub-word vocabulary is that it solves the vocabulary problem. It can split words and deal with individual characters which is especially useful when dealing with plurals. For example: 'chickens' can be split into 'chicken' and 's'\n",
        "\n",
        "The frequency model is more likely to leave out rarely used words which might impact its performance if a text has more less-frequently used words. It might not be able to capture the full meaning of the text. "
      ],
      "metadata": {
        "id": "zxOw8NaoSlq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "VOCABULARY_LENGTH = 8000\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    # The target vocabulary size\n",
        "    vocab_size = VOCABULARY_LENGTH,\n",
        "    # Reserved tokens that must be included in the vocabulary\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    # Arguments for `text.BertTokenizer`\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "    learn_params={},\n",
        ")\n",
        "\n",
        "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
        "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n"
      ],
      "metadata": {
        "id": "rRkwOd1vg81M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(transcript, recap):\n",
        "  return tf.strings.lower(transcript), tf.strings.lower(recap)\n",
        "\n",
        "\n",
        "def add_start_end(ragged):\n",
        "  count = ragged.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], START)\n",
        "  ends = tf.fill([count,1], END)\n",
        "  return tf.concat([starts, ragged, ends], axis=1)\n",
        "\n",
        "\n",
        "def get_transcript(transcript, recap):\n",
        "  return transcript"
      ],
      "metadata": {
        "id": "SNADwX1weAeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a vocabulary called `en_vocab` by invoking `bert_vocab_from_dataset()` that takes in a dataset the lowercased trasncripts and the `**bert_vocab_args` created above. This should complete in less than 10 minutes. "
      ],
      "metadata": {
        "id": "izOE_AJ1U1E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    ds_train.batch(1000).map(to_lowercase).map(get_transcript).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "id": "87IvnC5rmLAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_vocab[:30])\n",
        "print(en_vocab[100:110])\n",
        "print(en_vocab[1000:1010])\n",
        "print(en_vocab[-10:])"
      ],
      "metadata": {
        "id": "nFW_7pl9162m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save the vocabulary path to gDrive\n",
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)\n",
        "\n",
        "write_vocab_file(vocabulary_file, en_vocab)\n",
        "!cat \"$vocabulary_file\"\n",
        "print('Saved the vocabulary to %s.' %vocabulary_file)"
      ],
      "metadata": {
        "id": "KYmlJ5oI5IHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the vocabulary path from gDrive\n",
        "en_tokenizer = text.BertTokenizer(vocabulary_file, **bert_tokenizer_params)"
      ],
      "metadata": {
        "id": "K_S-kl6bYbOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Determine the max input transcript token length\n",
        "\n",
        "The translation task has two vocabularies, and the maximum token lengths can be the same (the input and output languages have similar lengths). However, in summarization, the input transcript is significantly longer than the output recap, but both can use the same vocabulary. These are two adaptations you will have to add to make the translation architecture into a summarization architecture.\n",
        "\n",
        "While you can use a reasonable recap (output) length of 64 tokens, you have to figure out a good length for the transcript (input) tokens. Create a histogram of token lengths from a sample of the transcripts to get a feeling for how many tokens cover most transcripts. The max length should be about 17,000 tokens. However, judging by the histogram, you can choose a smaller encoding length to cover most cases instead of all cases. \n"
      ],
      "metadata": {
        "id": "8jXjwTOLSnVB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRbke-iaaHFI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for transcript_example, recap_example in ds_train.batch(256).map(to_lowercase):\n",
        "  transcript_tokens = en_tokenizer.tokenize(transcript_example)\n",
        "  lengths = transcript_tokens.row_lengths()\n",
        "  break\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ucA1q3GaK_n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.hist(lengths, np.linspace(0, 20000, 50))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim(), )\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the `MAX_TOKENS_TRANSCRIPT` based on your histogram analysis. You should replace `MAX_TOKENS` with `MAX_TOKENS_TRANSCRIPT` and `MAX_TOKENS_RECAP`.\n",
        "\n",
        "**Note**: For this assignment, we'll start with a very small max length. You should first get results (even if they're not great) with a much smaller input length (`MAX_TOKENS_TRANSCRIPT`) than the transcripts to ensure that you have enough resources to execute the assignment completely. Once you have completed the full pass to the results, you can come back and change the architecture and the input data for extra credit."
      ],
      "metadata": {
        "id": "xVbUAPK2UbUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS_TRANSCRIPT = 10000\n",
        "MAX_TOKENS_RECAP = 64"
      ],
      "metadata": {
        "id": "Tl0hx4XoUVqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, adapt the transation `prepare_batch()` from the transformer tutorial colab, changing the input arguments from `pt`, `en` to `transcript`, `recap`, and output args from `(pt, en_inputs), en_labels` to `(transcript_tokens, recap_inputs), recap_labels`."
      ],
      "metadata": {
        "id": "F8Q4CvGcdHQu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6shgzEck3FiV"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def prepare_batch(transcript, recap):\n",
        "    \n",
        "    transcript_tokens = en_tokenizer.tokenize(transcript)\n",
        "\n",
        "    transcript_tokens = transcript_tokens.merge_dims(-2,-1)   \n",
        "\n",
        "    transcript_tokens = transcript_tokens[:, :MAX_TOKENS_TRANSCRIPT]    # Trim to MAX_TOKENS.\n",
        "    transcript_tokens = add_start_end(transcript_tokens)\n",
        "\n",
        "    transcript_tokens = transcript_tokens.to_tensor()  # Convert to 0-padded dense Tensor\n",
        "\n",
        "    recap_tokens = en_tokenizer.tokenize(recap)\n",
        "    recap_tokens = recap_tokens.merge_dims(-2,-1)\n",
        "      \n",
        "    recap_tokens = recap_tokens[:, :(MAX_TOKENS_RECAP+1)]\n",
        "    recap_tokens = add_start_end(recap_tokens)\n",
        "    recap_inputs = recap_tokens[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    recap_labels = recap_tokens[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return (transcript_tokens, recap_inputs), recap_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the same `make_batches()` from the tutorial. When you print out the examples, the transcripts should all have the same lengths, abd the recaps should all have the same lengths because the are truncated."
      ],
      "metadata": {
        "id": "MkLbnBvAekCg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUN_jLBTwNxk"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSswr5TKvoNM"
      },
      "outputs": [],
      "source": [
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(ds_train)\n",
        "val_batches = make_batches(ds_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title View an example of the input training data.\n",
        "for (transcript_tokens, recap_inputs), recap_labels in ds_train.batch(1024).map(prepare_batch).take(1):\n",
        "  print(transcript_tokens)\n",
        "  print(recap_inputs)\n",
        "  print(recap_labels)\n",
        "  "
      ],
      "metadata": {
        "id": "oulM1fP2E09V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2: Training Data\n",
        "2.1 Describe the significance of `(transcript_tokens, recap_inputs), recap_labels`. \n",
        "\n",
        "**ANSWER**\n",
        "These allow us to enforce \"teacher forcing\" because no matter the output, the input for the next timestep is the true value. At each input stage, the corresponding label is the ID of the token. \n",
        "\n",
        "2.2 Explain the tensor dimensions of each of the input examples. \n",
        "\n",
        "**ANSWERS**\n",
        "Each transcript batch has a shape of (1024,258) and each recap_labels with shape (1024, 66)\n",
        "\n",
        "2.3 What doe the individual value represent?\n",
        "\n",
        "**Answers**\n",
        "\n",
        "2.1 The starting value is the token id of start and the ending value is the end token id. The values between those two are the token ids for each word."
      ],
      "metadata": {
        "id": "SRcR05bpC2sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for  (transcript_tokens, recap_inputs), recap_labels in train_batches.take(1):\n",
        "  break\n",
        "\n",
        "print(transcript_tokens.shape)\n",
        "print(recap_inputs.shape)\n",
        "print(recap_labels.shape)"
      ],
      "metadata": {
        "id": "y-pHmiWSanHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apFeC-WWxzR4"
      },
      "outputs": [],
      "source": [
        "print(recap_inputs[0][:])\n",
        "print(recap_labels[0][:])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Define the Transformer\n",
        "\n",
        "This section should follow the steps from the [Transformer demo](https://www.tensorflow.org/text/tutorials/transformer) starting at “Define the components” and `positional_encoding()` step in the Transformer colab. Review each section and ensure that you are familiar with each part of the encoder and decoder of the Transformer.\n",
        "\n",
        "The end of the section, you should have a class called `Transformer` that is essentially the same as in the tutorial. (No need to copy over the pictures or the text fields, just the code blocks.)\n",
        "\n",
        "Note that the shapes should reflect your transcript and recap configurations (i.e., they shold not be the same as with the translator). \n",
        "\n"
      ],
      "metadata": {
        "id": "aR75gkkwbfVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional encoding\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "D2R9xnDzwNZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ],
      "metadata": {
        "id": "elI7NcXEwPN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Base Attention Layer\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "GOUWpFknwRPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Attention Layer\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "mY3DQpwnwUlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Attention Layer\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "hIn0Z-RIwWoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Casual Self Attention Layer\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9hLDXegVwadh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed Forward Network\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ],
      "metadata": {
        "id": "mkkSWnVCwepQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Layer\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "F0g5kvwzwjgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "wnosOdq6wnkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Layer\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ],
      "metadata": {
        "id": "AHXs1IpFwo27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ],
      "metadata": {
        "id": "BjrMclGTwsBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ],
      "metadata": {
        "id": "aBIHub1iwsrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3 Architecture\n",
        "Review the [PEGASUS](https://arxiv.org/abs/1912.08777) summarization architecture. \n",
        "\n",
        "3.1 What are the primary architectural differences between this architecture and PEGASUS?\n",
        "\n",
        "**Answer**\n",
        "In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\n",
        "Our model uses the base transformerl model architecture where pre-processing is similar to language translation model.\n",
        "\n",
        "3.2 How many parameters do $\\text{PEGASUS}_{BASE}$ and $\\text{PEGASUS}_{LARGE}$ and how many parameters does your model have?\n",
        "\n",
        "\n",
        "**Answer**\n",
        "PEGASUS base has 223M parameters and PEGASUS large has 568M parameters. Our model has 10,468,672 parameters."
      ],
      "metadata": {
        "id": "7gtCJKyiHwRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can set the same parameters as the tutorial, or fine tune the parameters."
      ],
      "metadata": {
        "id": "Olf7O9zkjqzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzyo6KDfVyhl"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiysUa--4tOU"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=VOCABULARY_LENGTH,\n",
        "    target_vocab_size=VOCABULARY_LENGTH,\n",
        "    dropout_rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8eO85hpFHmE"
      },
      "outputs": [],
      "source": [
        "output = transformer((transcript_tokens, recap_inputs))\n",
        "\n",
        "print(transcript_tokens.shape)\n",
        "print(recap_inputs.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsUPhlfEtOjn"
      },
      "outputs": [],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYQdOO1axwEI"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "  def get_config(self):\n",
        "    \"\"\"Must implement get_config() to enable checkpointing.\"\"\"\n",
        "    config = {\n",
        "    'd_model': self.d_model,\n",
        "    'warmup_steps': self.warmup_steps,\n",
        "\n",
        "     }\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r4scdulztRx"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, compile and train the transformer. This may take hours, but you can call fit multiple times on the same model to give you a sense of the progress the model is making on the summarization task."
      ],
      "metadata": {
        "id": "tjz6hn0Ij9FB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Una1v0hDlIsT"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have already done some training, and want to recover the transformer from checkpoint, the block below will load up the existing transformer."
      ],
      "metadata": {
        "id": "dYhOUkmSa6ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  transformer.load_weights(checkpoint_filepath)\n",
        "  print(\"Loaded a checkpoint from %s.\" %checkpoint_filepath)\n",
        "\n",
        "except OSError:\n",
        "  print('No checkpoints found, and not able to load a model.')"
      ],
      "metadata": {
        "id": "l9tKWN65a4fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Train the Summarizer\n",
        "\n",
        "**Note that `transfomer.fit()` will take several hours to execute, even with a TPU runtime.** Here, we add in saving checkpoints after every episode. This way, if the colab runtime disconnects, you can recover the trained transformer entirely by loading up the weights stored in the checkpoints."
      ],
      "metadata": {
        "id": "XELQEDnskjTS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg35qKvVlctp"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "NUM_EPOCHS = 100\n",
        "try:\n",
        "  transformer.load_weights(checkpoint_filepath)\n",
        "  print(\"Loaded a checkpoint from %s.\" %checkpoint_filepath)\n",
        "\n",
        "except OSError:\n",
        "  print('No checkpoints found. Starting a new transformer model')\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True, \n",
        "    monitor='val_masked_accuracy', # Save when the validation masekd accuracy improves\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "transformer.fit(train_batches,\n",
        "                epochs=NUM_EPOCHS,\n",
        "                validation_data=val_batches, callbacks=[model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some fairly minor modifications were required to change the `Translator` into the `Summarizer` class. Only one vocabulary is used and the tokenizer here uses a `detokenize()` method to convert from index to token string. "
      ],
      "metadata": {
        "id": "g5VNNj1HeDS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY_uXsOhSmbb"
      },
      "outputs": [],
      "source": [
        "class Summarizer(tf.Module):\n",
        "  def __init__(self, tokenizer, transformer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS_RECAP):\n",
        " \n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizer.tokenize(sentence)\n",
        "    sentence_tokens = sentence.merge_dims(-2,-1)\n",
        "    sentence_tokens = add_start_end(sentence_tokens)\n",
        "  \n",
        "    sentence = sentence_tokens.to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is English, initialize the output with the\n",
        "    # English `[START]` token.\n",
        "    # start_end = self.tokenizer.tokenize([''])[0]\n",
        "    start_end = tf.convert_to_tensor(np.array([START,END]), dtype=tf.int64)\n",
        "  \n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "   \n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "  \n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "     \n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # We had to add in detokenise for this tokenizer. The to_tensor() call \n",
        "    # converts a RaggedTensor into a normal Tensor. \n",
        "    tokens = self.tokenizer.detokenize(output).to_tensor()\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    tokens = np.array([b.decode(\"utf-8\") for b in tokens[0].numpy()])  # Shape: `()`.\n",
        "\n",
        "    text = \" \".join(tokens[1:-1])\n",
        "    \n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores  \n",
        "\n",
        "    return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NjbvpHUTEia"
      },
      "outputs": [],
      "source": [
        "#@title Create a summarizer by combining the tokenizer and the transformer\n",
        "summarizer = Summarizer(en_tokenizer, transformer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9CEm4cuTGtw"
      },
      "outputs": [],
      "source": [
        "#@title Show an example prediction and compare it to the ground truth\n",
        "def print_summary(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {\" \".join([ tokens])}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')\n",
        "\n",
        "\n",
        "\n",
        "sentence = 'In the media immersion lab (Spinner is listening to Craig\\'s new CD.)'\n",
        "target_recap = 'Downtown Sasquatch has a wedding gig, and Craig decides to put all of his focus on the music, but things get complicated when Manny and Ellie vie for Craig\\'s attention. Meanwhile, Joey has a date with the bride\\'s sister but wonders if he\\'s too old for her when he overhears her friends gossiping about him.'\n",
        "transcript = 'In Craig\\'s garage, the band is practicing Craig: (Singing) Well somebody better stop me \\'cause my feet don\\'t touch the ground. I can\\'t keep my hands off of you and there\\'s nothing I can do- (The sound is really off and Craig gives them a weird look.) Craig: (Singing) I can\\'t keep my hands off of you. Craig: Alright stop. Stop! Jimmy, look unless your last name is Hendrix and you\\'ve come back to rock us from the grave. No solos while I\\'m singing! Marco: We\\'re never gonna get a wedding job.'\n",
        "summarized_text, translated_tokens, attention_weights = summarizer(  tf.constant(transcript))\n",
        "print(summarized_text)\n",
        "print_summary(transcript, summarized_text, target_recap)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Evaluate the summarizer"
      ],
      "metadata": {
        "id": "bviBnCtUmB1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example of ROUGE-1 and ROUGE-L\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                      'The quick brown dog jumps on the log.')\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "AKQfrkppmeBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKlxYO0JTXzD"
      },
      "outputs": [],
      "source": [
        "#@title Some utilities for plotting and displaying results\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "\n",
        "def transcript_to_tokens(transcript):\n",
        "  in_tokens = tf.convert_to_tensor([transcript])\n",
        "  in_tokens = en_tokenizer.tokenize(in_tokens)\n",
        "\n",
        "  in_tokens = en_tokenizer.detokenize(in_tokens).to_tensor()[0]\n",
        "  return in_tokens.reshape(in_tokens.shape[0])\n",
        "\n",
        "def get_attention_head(attention_weights, head_id):\n",
        "  attention_heads = tf.squeeze(attention_weights, 0)\n",
        "  return attention_heads[head_id]\n",
        "  \n",
        "\n",
        "def plot_attention_head(in_tokens, summary_tokens, attention):\n",
        "  # The model didn't generate `<START>` in the output. Skip it.\n",
        "  summary_tokens = summary_tokens[1:]\n",
        "\n",
        "  n_rows = len(summary_tokens)\n",
        "  n_cols = len(in_tokens)\n",
        "  fig, ax = plt.subplots(figsize=(int(n_cols/3),int(n_rows/3)))\n",
        "  \n",
        "  ax.matshow(attention[:n_rows, :n_cols])\n",
        "  ax.set_xticks(range(n_cols))\n",
        "  ax.set_yticks(range(n_rows))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels( labels, rotation=90)\n",
        "\n",
        "  labels = summary_tokens\n",
        "  ax.set_yticklabels(labels)\n",
        "  \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head = 1\n",
        "\n",
        "\n",
        "df_test = tfds.as_dataframe(ds_test.shuffle(1000000).take(20000), ds_info)\n",
        "\n",
        "plot_attention_weights = True\n",
        "results  = []\n",
        "for i in range(len(df_test)):\n",
        "  print(\"Example %d of %d: \" %(i + 1, len(df_test)))\n",
        "  recap = df_test.iloc[i]['recap'].decode(\"utf-8\")\n",
        "  transcript = df_test.iloc[i]['transcript'][:MAX_TOKENS_TRANSCRIPT].decode(\"utf-8\").replace('\\n', ' ')\n",
        "  summarized_text, summary_tokens, attention_weights = summarizer(  tf.constant(transcript))\n",
        "  print_summary(transcript, summarized_text, recap)\n",
        "  score = scorer.score(target = recap, prediction = summarized_text)\n",
        "  result = {'ROUGE1_precision': score['rouge1'].precision, \n",
        "                  'ROUGE1_recall': score['rouge1'].recall,\n",
        "                  'ROUGE1_fmeasure': score['rouge1'].fmeasure, \n",
        "                  'ROUGEL_precision': score['rougeL'].precision, \n",
        "                  'ROUGEL_recall': score['rougeL'].recall,\n",
        "                  'ROUGEL_fmeasure': score['rougeL'].fmeasure,\n",
        "                  'ground_truth': recap,\n",
        "                  'prediction': summarized_text,\n",
        "                  'input': transcript\n",
        "             }\n",
        "  # Plot out the ROUGE scores\n",
        "  for key, value in result.items():\n",
        "    if key.startswith('ROUGE'):\n",
        "      print(\"%12s: %3.2f\" %(key, value))\n",
        "\n",
        "  results.append(result)\n",
        "\n",
        "  if plot_attention_weights:\n",
        "    in_tokens = transcript_to_tokens(transcript)\n",
        "    attention = get_attention_head(attention_weights, 1)\n",
        "    plot_attention_head(in_tokens, summary_tokens, attention )\n",
        "\n",
        "\n",
        "\n",
        "test_results = pd.DataFrame(results)\n",
        "print(test_results.describe())"
      ],
      "metadata": {
        "id": "betQTW-4fUfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4 Evaluation\n",
        "\n",
        "4.1 What are the highest ROUGE 1/L scores (precision, recall, f-measure) your model was able to attain?\n",
        "\n",
        "**ANSWER**\n",
        "The highest ROUGE 1 precision score was 0.52 and the ROUGE L precision score was 0.26 after increasing MAX_TOKENS_TRANSCRIPT to 1024. The recall scores were low at around 0.18 and f-measure was 0.14\n",
        "\n",
        "4.2 Review at least ten summaries manually. Are the summaries gramatically correct? Do the summaries capture the meaning of the associated transcripts? \n",
        "\n",
        "**ANSWER**\n",
        "There are a lot of grammatical errors. The summaries lose a lot of the meaning of the associated transcripts mid sentence.\n",
        "\n",
        "4.3 How well do the ROUGE metrics capture the quality of a summary? \n",
        "\n",
        "**ANSWER**\n",
        "The summaries with higher scores have lesser grammar errors. They also don't lose meaning as often.\n",
        "\n",
        "4.4 Review the attention plots. Provide three examples where the attention links a word from the transcript to the summary.\n",
        "\n",
        "**ANSWER** \n",
        "In the summary for the Vampire Diaries \"I\" is associated with three characters, Damon, Elena, and Stefan."
      ],
      "metadata": {
        "id": "Vj1SI2BbTtSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5 [OPTIONAL up to 3 semester points] Architecture Enhancement  \n",
        "\n",
        "Apply at least two parameter changes that yield significantly better results based on ROUGE 1 or ROUGE L against the original design. (Running for more iterations does not count.)\n",
        "\n",
        "5.1 What parameter changes did you make?\n",
        "\n",
        "**ANSWER**\n",
        "I increased MAX_TOKENS_TRANSCRIPT to over 1000 and tested d_model = 256 and 512\n",
        "\n",
        "5.2 How much improvment in ROUGE scores did you observe?\n",
        "\n",
        "**ANSWER**\n",
        "ROUGE 1 precision score went up to 0.74\n",
        "\n",
        "5.3 Provide a brief explanation of why the changes yieled better performance.\n",
        "\n",
        "**ANSWER**\n",
        "Increasing the MAX_TOKENS_TRANSCRIPT allows us to take into account the whole transcript. Increasing the d_model allows us to retain the context for each example for longer. \n",
        "\n"
      ],
      "metadata": {
        "id": "MRyEWGbEWDJN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "collapsed_sections": [
        "MRyEWGbEWDJN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
